{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharlottePrimiceri/VP_Project/blob/main/segmented_video/Segmented_video_Mask_RCNN_and_Pytorch_UNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Segmented video generation applying the pre-trained Mask-RCNN and our trained U-Net.**\n",
        "\n",
        "References:\n",
        "\n",
        "- Mask-RCNN:\n",
        "https://github.com/facebookresearch/detectron2.git@5aeb252b194b93dc2879b4ac34bc51a31b5aee13'\n",
        "- Video taken from: https://www.cityscapes-dataset.com/"
      ],
      "metadata": {
        "id": "rRARDshbAGqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install detectron2, import libraries, connect to google drive, set device."
      ],
      "metadata": {
        "id": "9dOR3H6C_x-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch --quiet"
      ],
      "metadata": {
        "id": "0Sa0HJZCCazs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision --quiet"
      ],
      "metadata": {
        "id": "iys5vRhCCXAu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install scikit-image --quiet"
      ],
      "metadata": {
        "id": "MmRf4zpuCl1U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install opencv-python --quiet"
      ],
      "metadata": {
        "id": "WW6CMTtrCoHy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ri9NL6pC5kc",
        "outputId": "fa0b429e-1016-4e2c-dd29-083d241a64c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4lfLXNu_m-G",
        "outputId": "805ae282-bf8b-4571-f48f-369379c8773f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git@5aeb252b194b93dc2879b4ac34bc51a31b5aee13' --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1JJ1paoBzIr",
        "outputId": "e8eb86b5-f78a-4c4d-ed3b-1b167610bdfe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import detectron2\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "metadata": {
        "id": "H3BsMVsmB814"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detectron2 configuration"
      ],
      "metadata": {
        "id": "HKicnU8HCFj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = get_cfg()\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "metadata": {
        "id": "eF62FXATGE8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f17774c1-c27c-4366-92b7-e98b9b8ed90e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_f10217.pkl: 178MB [00:01, 136MB/s]                           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/stuttgart_02_output/\" # original frames\n",
        "segmented_video_path = \"/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/segmented_video_mask/\" # segmented frames\n",
        "built_video_path = \"/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/built_video_mask/\"\n",
        "\n",
        "\n",
        "def generate_video(img_folder, video_name, out_path):\n",
        "\n",
        "    images = [img for img in os.listdir(img_folder)]\n",
        "    frame = cv2.imread(os.path.join(img_folder, images[0]))\n",
        "    height, width, layers = frame.shape\n",
        "\n",
        "    os.chdir(out_path)\n",
        "\n",
        "    video = cv2.VideoWriter(video_name, 0, 1, (width, height))   # try 2,3,4 al posto di 1 per velocizzare\n",
        "\n",
        "    for image in images:\n",
        "        video.write(cv2.imread(os.path.join(img_folder, image)))\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "    video.release()\n",
        "\n",
        "\n",
        "for image in os.listdir(video_path):\n",
        "    image_path = os.path.join(video_path, image)\n",
        "    im = cv2.imread(image_path)\n",
        "\n",
        "    # here we segment the image with the pre-trained Mask-RCNN\n",
        "    pred = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "    out = v.draw_instance_predictions(pred[\"instances\"].to(\"cpu\"))\n",
        "    path = os.path.join(segmented_video_path, image)\n",
        "    cv2.imwrite(path, out.get_image()[:, :, ::-1])\n",
        "\n",
        "# Build video from folder segmented_video\n",
        "generate_video(segmented_video_path, \"segmented_video_1.avi\", built_video_path)\n"
      ],
      "metadata": {
        "id": "2JRYx6RIdLw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch-UNet PREDICT SCRIPT"
      ],
      "metadata": {
        "id": "S8WU0MSxrvSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torchvision import transforms, utils\n",
        "import torchvision.transforms.functional as TF\n",
        "from collections import namedtuple\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.utils as utils\n",
        "import torch.nn.init as init\n",
        "import torch.utils.data as data\n",
        "import torchvision.utils as v_utils\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import pdb\n"
      ],
      "metadata": {
        "id": "3Toed3o8BBJc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############# MODEL BLOCKS ######################################\n",
        "\n",
        "def conv_block(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "        act_fn,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def conv_trans_block(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_dim, out_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "        act_fn,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def maxpool():\n",
        "    pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "    return pool\n",
        "\n",
        "\n",
        "def conv_block_2(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        conv_block(in_dim, out_dim, act_fn),\n",
        "        nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def conv_block_3(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        conv_block(in_dim, out_dim, act_fn),\n",
        "        conv_block(out_dim, out_dim, act_fn),\n",
        "        nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "    )\n",
        "    return model\n",
        "\n",
        "################## MODEL #################################\n",
        "\n",
        "\n",
        "class UnetGenerator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, num_filter):\n",
        "        super(UnetGenerator, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.num_filter = num_filter\n",
        "        act_fn = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "        self.down_1 = conv_block_2(self.in_dim, self.num_filter, act_fn)\n",
        "        self.pool_1 = maxpool()\n",
        "        self.down_2 = conv_block_2(self.num_filter*1, self.num_filter*2, act_fn)\n",
        "        self.pool_2 = maxpool()\n",
        "        self.down_3 = conv_block_2(self.num_filter*2, self.num_filter*4, act_fn)\n",
        "        self.pool_3 = maxpool()\n",
        "        self.down_4 = conv_block_2(self.num_filter*4, self.num_filter*8, act_fn)\n",
        "        self.pool_4 = maxpool()\n",
        "\n",
        "        self.bridge = conv_block_2(self.num_filter*8, self.num_filter*16, act_fn)\n",
        "\n",
        "        self.trans_1 = conv_trans_block(self.num_filter*16, self.num_filter*8, act_fn)\n",
        "        self.up_1 = conv_block_2(self.num_filter*16, self.num_filter*8, act_fn)\n",
        "        self.trans_2 = conv_trans_block(self.num_filter*8, self.num_filter*4, act_fn)\n",
        "        self.up_2 = conv_block_2(self.num_filter*8, self.num_filter*4, act_fn)\n",
        "        self.trans_3 = conv_trans_block(self.num_filter*4, self.num_filter*2, act_fn)\n",
        "        self.up_3 = conv_block_2(self.num_filter*4, self.num_filter*2, act_fn)\n",
        "        self.trans_4 = conv_trans_block(self.num_filter*2, self.num_filter*1, act_fn)\n",
        "        self.up_4 = conv_block_2(self.num_filter*2, self.num_filter*1, act_fn)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(self.num_filter, self.out_dim, 3, 1, 1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        down_1 = self.down_1(input)\n",
        "        pool_1 = self.pool_1(down_1)\n",
        "        down_2 = self.down_2(pool_1)\n",
        "        pool_2 = self.pool_2(down_2)\n",
        "        down_3 = self.down_3(pool_2)\n",
        "        pool_3 = self.pool_3(down_3)\n",
        "        down_4 = self.down_4(pool_3)\n",
        "        pool_4 = self.pool_4(down_4)\n",
        "\n",
        "        bridge = self.bridge(pool_4)\n",
        "\n",
        "        trans_1 = self.trans_1(bridge)\n",
        "        concat_1 = torch.cat([trans_1, down_4], dim=1)\n",
        "        up_1 = self.up_1(concat_1)\n",
        "        trans_2 = self.trans_2(up_1)\n",
        "        concat_2 = torch.cat([trans_2, down_3], dim=1)\n",
        "        up_2 = self.up_2(concat_2)\n",
        "        trans_3 = self.trans_3(up_2)\n",
        "        concat_3 = torch.cat([trans_3, down_2], dim=1)\n",
        "        up_3 = self.up_3(concat_3)\n",
        "        trans_4 = self.trans_4(up_3)\n",
        "        concat_4 = torch.cat([trans_4, down_1], dim=1)\n",
        "        up_4 = self.up_4(concat_4)\n",
        "\n",
        "        out = self.out(up_4)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "LNe9M1xor0DC"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "generator = UnetGenerator(3, 4, 64).to(device)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--num_gpu\", type=int, default=0, help=\"number of gpus\")\n",
        "parser.add_argument(\"-f\", required=False)\n",
        "args = parser.parse_args()\n",
        "\n",
        "# load pretrained model if it is there\n",
        "print(\"loading unet model...\")\n",
        "file_model = '/content/drive/MyDrive/VPPROJECT/EUREKA/checkpoint_UNet/unet.pkl'\n",
        "if os.path.isfile(file_model):\n",
        "    generator = torch.load(file_model, map_location=torch.device('cpu'))\n",
        "    print(\"    - model restored from file....\")\n",
        "    print(\"    - filename = %s\" % file_model)\n",
        "else:\n",
        "    print(\"unable to load unet.pkl model file\")\n",
        "    exit()\n",
        "\n",
        "print(\"\\n(predicting all the segmented frames will take a while)...\")\n",
        "\n",
        "# load images from folder\n",
        "folder_path = \"/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/stuttgart_02_output/\"\n",
        "segmented_images = []\n",
        "i = 0\n",
        "for file in os.listdir(folder_path):\n",
        "      i+=1\n",
        "      img_path = os.path.join(folder_path, file)\n",
        "      image = Image.open(img_path).convert('RGB')\n",
        "      transform_image = torchvision.transforms.Compose([\n",
        "                                    torchvision.transforms.Resize(size=(128, 256), interpolation=Image.BILINEAR),\n",
        "                                    torchvision.transforms.ToTensor()])\n",
        "      image = transform_image(image)\n",
        "      image = torch.unsqueeze(image, 0)\n",
        "      x = Variable(image).to(device)\n",
        "      y = generator.forward(x)\n",
        "      pred_class = torch.zeros((y.size()[0], y.size()[2], y.size()[3]))\n",
        "      for idx in range(0, y.size()[0]):\n",
        "          pred_class[idx] = torch.argmax(y[idx], dim=0).to(device).int()\n",
        "      pred_class = pred_class.unsqueeze(1).float()\n",
        "      v_utils.save_image(pred_class.float()/y.size()[1], '/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/predict_segmented_unet/gen_image_{}.png'.format(i))"
      ],
      "metadata": {
        "id": "eSAwrKnjBh0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video built with Pytorch-Unet-revisited predictions"
      ],
      "metadata": {
        "id": "W69Dpahhqpi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_video(img_folder, video_name, out_path):\n",
        "\n",
        "    images = [img for img in os.listdir(img_folder)]\n",
        "    frame = cv2.imread(os.path.join(img_folder, images[0]))\n",
        "    height, width, layers = frame.shape\n",
        "\n",
        "    os.chdir(out_path)\n",
        "\n",
        "    video = cv2.VideoWriter(video_name, 0, 1, (width, height))   # try 2,3,4 al posto di 1 per velocizzare\n",
        "\n",
        "    for image in images:\n",
        "        video.write(cv2.imread(os.path.join(img_folder, image)))\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "    video.release()\n",
        "\n",
        "    return 0"
      ],
      "metadata": {
        "id": "AxRcmDkTJ5ev"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build video from folder segmented_video\n",
        "segmented_video_unet_path = \"/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/predict_segmented_unet/\"\n",
        "built_video_unet_path = \"/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/built_video_unet/\"\n",
        "generate_video(segmented_video_unet_path, \"/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/segmented_video_unet.avi\", built_video_unet_path)"
      ],
      "metadata": {
        "id": "JRePBYVB1SVO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}