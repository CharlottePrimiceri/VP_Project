{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharlottePrimiceri/VP_Project/blob/main/segmented_video/Segmented_video_Mask_RCNN_and_Pytorch_UNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Segmented video generation applying the pre-trained Mask-RCNN and our trained U-Nets**"
      ],
      "metadata": {
        "id": "rRARDshbAGqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch --quiet"
      ],
      "metadata": {
        "id": "0Sa0HJZCCazs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision --quiet"
      ],
      "metadata": {
        "id": "iys5vRhCCXAu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install scikit-image --quiet"
      ],
      "metadata": {
        "id": "MmRf4zpuCl1U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install opencv-python --quiet"
      ],
      "metadata": {
        "id": "WW6CMTtrCoHy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install Pillow --quiet"
      ],
      "metadata": {
        "id": "-Ri9NL6pC5kc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm --quiet"
      ],
      "metadata": {
        "id": "qQQiw0qYIlq9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4lfLXNu_m-G",
        "outputId": "3a9fb106-7873-46b2-b746-d54fc5382240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib as plt\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, utils\n",
        "import torchvision\n",
        "import torch.utils as utils\n",
        "import torch.nn.init as init\n",
        "import torchvision.utils as v_utils\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'"
      ],
      "metadata": {
        "id": "MJv87EsWLCA6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define useful functions"
      ],
      "metadata": {
        "id": "oRk__VFddsFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sort frames in order to create the video from the right sequence of frames\n",
        "\n",
        "def frame_sort_key(filename):\n",
        "    return int(filename.split('.')[0][20:26])\n",
        "\n",
        "def generate_video(img_folder, video_name, out_path):\n",
        "\n",
        "    images = [img for img in os.listdir(img_folder)]\n",
        "    images.sort(key=frame_sort_key)\n",
        "    frame = cv2.imread(os.path.join(img_folder, images[0]))\n",
        "    height, width, layers = frame.shape\n",
        "\n",
        "    os.chdir(out_path)\n",
        "\n",
        "    video = cv2.VideoWriter(video_name, 0, 10, (width, height))\n",
        "\n",
        "    for image in images:\n",
        "        video.write(cv2.imread(os.path.join(img_folder, image)))\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "    video.release()"
      ],
      "metadata": {
        "id": "AxRcmDkTJ5ev"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### function to rename image files in folders\n",
        "\n",
        "path_segmented_car = \"/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/predict_segmented_unet\"\n",
        "\n",
        "list_frames = [frame for frame in os.listdir(path_segmented_car) if frame.endswith(\".png\")]\n",
        "\n",
        "for i, list_frames in enumerate(list_frames, start=1):\n",
        "    new_filename = f\"{i:04d}.png\"\n",
        "\n",
        "    old_path = os.path.join(path_segmented_car, list_frames)\n",
        "    new_path = os.path.join(path_segmented_car, new_filename)\n",
        "\n",
        "    os.rename(old_path, new_path)\n",
        "\n",
        "    print(f\"Renamed '{list_frames}' to '{new_filename}'\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Tm4tgsDrvXUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build video with Detectron2"
      ],
      "metadata": {
        "id": "HKicnU8HCFj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git@5aeb252b194b93dc2879b4ac34bc51a31b5aee13' --quiet"
      ],
      "metadata": {
        "id": "F1JJ1paoBzIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import detectron2\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "metadata": {
        "id": "H3BsMVsmB814"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detectron2 configuration"
      ],
      "metadata": {
        "id": "jp8ouKtkdMCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = get_cfg()\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "metadata": {
        "id": "eF62FXATGE8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate segmented frames"
      ],
      "metadata": {
        "id": "jzabFXPFeuIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_path = \"/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/stuttgart_02\" # original frames\n",
        "segmented_frames_path = \"/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/mask_segm_frames/\" # segmented frames\n",
        "built_video_path = \"/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/mask_built_video/\" # complete video\n",
        "\n",
        "for image in os.listdir(images_path):\n",
        "    image_path = os.path.join(images_path, image)\n",
        "    im = cv2.imread(image_path)\n",
        "\n",
        "    # here we segment the image with the pre-trained Mask-RCNN\n",
        "    pred = predictor(im)\n",
        "    v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "    out = v.draw_instance_predictions(pred[\"instances\"].to(\"cpu\"))\n",
        "    path = os.path.join(segmented_frames_path, image)\n",
        "    cv2.imwrite(path, out.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "2JRYx6RIdLw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build video"
      ],
      "metadata": {
        "id": "AJwpn3mIe3c9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build video from folder segmented_video\n",
        "generate_video(segmented_frames_path, \"segmented_video_1.avi\", built_video_path)\n",
        "print(\"The segmented video has been generated now!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0dYCfXqnFjQ",
        "outputId": "f375cc39-b412-4b76-fad1-12409da1474c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The segmented video has been generated now!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single-channel Pytorch-UNet PREDICT SCRIPT"
      ],
      "metadata": {
        "id": "S8WU0MSxrvSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload Unet model"
      ],
      "metadata": {
        "id": "FsVsndMqcTA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############# MODEL BLOCKS ######################################\n",
        "\n",
        "def conv_block(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "        act_fn,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def conv_trans_block(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_dim, out_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "        act_fn,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def maxpool():\n",
        "    pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "    return pool\n",
        "\n",
        "\n",
        "def conv_block_2(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        conv_block(in_dim, out_dim, act_fn),\n",
        "        nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def conv_block_3(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        conv_block(in_dim, out_dim, act_fn),\n",
        "        conv_block(out_dim, out_dim, act_fn),\n",
        "        nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "    )\n",
        "    return model\n",
        "\n",
        "################## MODEL #################################\n",
        "\n",
        "\n",
        "class UnetGenerator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, num_filter):\n",
        "        super(UnetGenerator, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.num_filter = num_filter\n",
        "        act_fn = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "        self.down_1 = conv_block_2(self.in_dim, self.num_filter, act_fn)\n",
        "        self.pool_1 = maxpool()\n",
        "        self.down_2 = conv_block_2(self.num_filter*1, self.num_filter*2, act_fn)\n",
        "        self.pool_2 = maxpool()\n",
        "        self.down_3 = conv_block_2(self.num_filter*2, self.num_filter*4, act_fn)\n",
        "        self.pool_3 = maxpool()\n",
        "        self.down_4 = conv_block_2(self.num_filter*4, self.num_filter*8, act_fn)\n",
        "        self.pool_4 = maxpool()\n",
        "\n",
        "        self.bridge = conv_block_2(self.num_filter*8, self.num_filter*16, act_fn)\n",
        "\n",
        "        self.trans_1 = conv_trans_block(self.num_filter*16, self.num_filter*8, act_fn)\n",
        "        self.up_1 = conv_block_2(self.num_filter*16, self.num_filter*8, act_fn)\n",
        "        self.trans_2 = conv_trans_block(self.num_filter*8, self.num_filter*4, act_fn)\n",
        "        self.up_2 = conv_block_2(self.num_filter*8, self.num_filter*4, act_fn)\n",
        "        self.trans_3 = conv_trans_block(self.num_filter*4, self.num_filter*2, act_fn)\n",
        "        self.up_3 = conv_block_2(self.num_filter*4, self.num_filter*2, act_fn)\n",
        "        self.trans_4 = conv_trans_block(self.num_filter*2, self.num_filter*1, act_fn)\n",
        "        self.up_4 = conv_block_2(self.num_filter*2, self.num_filter*1, act_fn)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(self.num_filter, self.out_dim, 3, 1, 1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        down_1 = self.down_1(input)\n",
        "        pool_1 = self.pool_1(down_1)\n",
        "        down_2 = self.down_2(pool_1)\n",
        "        pool_2 = self.pool_2(down_2)\n",
        "        down_3 = self.down_3(pool_2)\n",
        "        pool_3 = self.pool_3(down_3)\n",
        "        down_4 = self.down_4(pool_3)\n",
        "        pool_4 = self.pool_4(down_4)\n",
        "\n",
        "        bridge = self.bridge(pool_4)\n",
        "\n",
        "        trans_1 = self.trans_1(bridge)\n",
        "        concat_1 = torch.cat([trans_1, down_4], dim=1)\n",
        "        up_1 = self.up_1(concat_1)\n",
        "        trans_2 = self.trans_2(up_1)\n",
        "        concat_2 = torch.cat([trans_2, down_3], dim=1)\n",
        "        up_2 = self.up_2(concat_2)\n",
        "        trans_3 = self.trans_3(up_2)\n",
        "        concat_3 = torch.cat([trans_3, down_2], dim=1)\n",
        "        up_3 = self.up_3(concat_3)\n",
        "        trans_4 = self.trans_4(up_3)\n",
        "        concat_4 = torch.cat([trans_4, down_1], dim=1)\n",
        "        up_4 = self.up_4(concat_4)\n",
        "\n",
        "        out = self.out(up_4)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "LNe9M1xor0DC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "generator = UnetGenerator(3, 4, 64).to(device)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--num_gpu\", type=int, default=0, help=\"number of gpus\")\n",
        "parser.add_argument(\"-f\", required=False)\n",
        "args = parser.parse_args()\n",
        "\n",
        "# load pretrained model if it is there\n",
        "print(\"loading unet model...\")\n",
        "file_model = '/content/drive/MyDrive/VPPROJECT/50epoch_nodepth/unet.pkl'\n",
        "if os.path.isfile(file_model):\n",
        "    generator = torch.load(file_model, map_location=torch.device('cpu'))\n",
        "    print(\"    - model restored from file....\")\n",
        "    print(\"    - filename = %s\" % file_model)\n",
        "else:\n",
        "    print(\"unable to load unet.pkl model file\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "sRSTPDCFcW-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate segmented frames with Unet"
      ],
      "metadata": {
        "id": "TRQUWSsxcZZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n(predicting all the segmented frames will take a while)...\")\n",
        "\n",
        "# load images from folder\n",
        "folder_path = \"/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/stuttgart_02\"\n",
        "segmented_images = []\n",
        "i = 0\n",
        "for file in os.listdir(folder_path):\n",
        "      i+=1\n",
        "      img_path = os.path.join(folder_path, file)\n",
        "      image = Image.open(img_path).convert('RGB')\n",
        "      transform_image = torchvision.transforms.Compose([\n",
        "                                    torchvision.transforms.Resize(size=(128, 256), interpolation=Image.BILINEAR),\n",
        "                                    torchvision.transforms.ToTensor()])\n",
        "      image = transform_image(image)\n",
        "      image = torch.unsqueeze(image, 0)\n",
        "      x = Variable(image).to(device)\n",
        "      y = generator.forward(x)\n",
        "      pred_class = torch.zeros((y.size()[0], y.size()[2], y.size()[3]))\n",
        "      for idx in range(0, y.size()[0]):\n",
        "          pred_class[idx] = torch.argmax(y[idx], dim=0).to(device).int()\n",
        "      pred_class = pred_class.unsqueeze(1).float()\n",
        "      v_utils.save_image(pred_class.float()/y.size()[1], '/content/drive/MyDrive/VPPROJECT/50epoch_nodepth/segmented_frames/gen_image_{}.png'.format(i))"
      ],
      "metadata": {
        "id": "eSAwrKnjBh0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build video"
      ],
      "metadata": {
        "id": "1IRi3xIecHyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build video from folder segmented_video\n",
        "segmented_video_unet_path = \"/content/drive/MyDrive/VPPROJECT/50epoch_nodepth/segmented_frames/\"\n",
        "built_video_unet_path = \"/content/drive/MyDrive/VPPROJECT/50epoch_nodepth/video_built/\"\n",
        "generate_video(segmented_video_unet_path, \"segmented_video_unet_nodepth.avi\", built_video_unet_path)\n",
        "print(\"The segmented video has been generated now!\")"
      ],
      "metadata": {
        "id": "J81bclaGqH6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build video with double-channel Pytorch-Unet"
      ],
      "metadata": {
        "id": "W69Dpahhqpi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload Unet model"
      ],
      "metadata": {
        "id": "Y_VAOvOhbKbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############# MODEL BLOCKS ######################################\n",
        "\n",
        "def conv_block(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "        act_fn,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def conv_trans_block(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_dim, out_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "        act_fn,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def maxpool():\n",
        "    pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "    return pool\n",
        "\n",
        "\n",
        "def conv_block_2(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        conv_block(in_dim, out_dim, act_fn),\n",
        "        nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def conv_block_3(in_dim, out_dim, act_fn):\n",
        "    model = nn.Sequential(\n",
        "        conv_block(in_dim, out_dim, act_fn),\n",
        "        conv_block(out_dim, out_dim, act_fn),\n",
        "        nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "################   UNET 1   #####################################\n",
        "\n",
        "class unet_without_last_layers_1(nn.Module):\n",
        "\n",
        "        def __init__(self, in_dim, out_dim, num_filter):\n",
        "                super(unet_without_last_layers_1, self).__init__()\n",
        "                self.in_dim = in_dim\n",
        "                self.out_dim = out_dim\n",
        "                self.num_filter = num_filter\n",
        "                act_fn = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "                self.down_1 = conv_block_2(self.in_dim, self.num_filter, act_fn)\n",
        "                self.pool_1 = maxpool()\n",
        "                self.down_2 = conv_block_2(self.num_filter*1, self.num_filter*2, act_fn)\n",
        "                self.pool_2 = maxpool()\n",
        "                self.down_3 = conv_block_2(self.num_filter*2, self.num_filter*4, act_fn)\n",
        "                self.pool_3 = maxpool()\n",
        "                self.down_4 = conv_block_2(self.num_filter*4, self.num_filter*8, act_fn)\n",
        "                self.pool_4 = maxpool()\n",
        "\n",
        "                self.bridge = conv_block_2(self.num_filter*8, self.num_filter*16, act_fn)\n",
        "\n",
        "                self.trans_1 = conv_trans_block(self.num_filter*16, self.num_filter*8, act_fn)\n",
        "                self.up_1 = conv_block_2(self.num_filter*16, self.num_filter*8, act_fn)\n",
        "                self.trans_2 = conv_trans_block(self.num_filter*8, self.num_filter*4, act_fn)\n",
        "                self.up_2 = conv_block_2(self.num_filter*8, self.num_filter*4, act_fn)\n",
        "                self.trans_3 = conv_trans_block(self.num_filter*4, self.num_filter*2, act_fn)\n",
        "                self.up_3 = conv_block_2(self.num_filter*4, self.num_filter*2, act_fn)\n",
        "                self.trans_4 = conv_trans_block(self.num_filter*2, self.num_filter*1, act_fn)\n",
        "                self.up_4 = conv_block_2(self.num_filter*2, self.num_filter*1, act_fn)\n",
        "\n",
        "        def forward(self, input):\n",
        "\n",
        "                down_1 = self.down_1(input)\n",
        "                pool_1 = self.pool_1(down_1)\n",
        "                down_2 = self.down_2(pool_1)\n",
        "                pool_2 = self.pool_2(down_2)\n",
        "                down_3 = self.down_3(pool_2)\n",
        "                pool_3 = self.pool_3(down_3)\n",
        "                down_4 = self.down_4(pool_3)\n",
        "                pool_4 = self.pool_4(down_4)\n",
        "\n",
        "                bridge = self.bridge(pool_4)\n",
        "\n",
        "                trans_1 = self.trans_1(bridge)\n",
        "                concat_1 = torch.cat([trans_1, down_4], dim=1)\n",
        "                up_1 = self.up_1(concat_1)\n",
        "                trans_2 = self.trans_2(up_1)\n",
        "                concat_2 = torch.cat([trans_2, down_3], dim=1)\n",
        "                up_2 = self.up_2(concat_2)\n",
        "                trans_3 = self.trans_3(up_2)\n",
        "                concat_3 = torch.cat([trans_3, down_2], dim=1)\n",
        "                up_3 = self.up_3(concat_3)\n",
        "                trans_4 = self.trans_4(up_3)\n",
        "                concat_4 = torch.cat([trans_4, down_1], dim=1)\n",
        "                up_4 = self.up_4(concat_4)\n",
        "\n",
        "                return up_4\n",
        "\n",
        "################   UNET 2   #####################################\n",
        "\n",
        "class unet_without_last_layers_2(nn.Module):\n",
        "\n",
        "        def __init__(self, in_dim, out_dim, num_filter):\n",
        "                super(unet_without_last_layers_2, self).__init__()\n",
        "                self.in_dim = in_dim\n",
        "                self.out_dim = out_dim\n",
        "                self.num_filter = num_filter\n",
        "                act_fn = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "                self.down_1 = conv_block_2(self.in_dim, self.num_filter, act_fn)\n",
        "                self.pool_1 = maxpool()\n",
        "                self.down_2 = conv_block_2(self.num_filter*1, self.num_filter*2, act_fn)\n",
        "                self.pool_2 = maxpool()\n",
        "                self.down_3 = conv_block_2(self.num_filter*2, self.num_filter*4, act_fn)\n",
        "                self.pool_3 = maxpool()\n",
        "                self.down_4 = conv_block_2(self.num_filter*4, self.num_filter*8, act_fn)\n",
        "                self.pool_4 = maxpool()\n",
        "\n",
        "                self.bridge = conv_block_2(self.num_filter*8, self.num_filter*16, act_fn)\n",
        "\n",
        "                self.trans_1 = conv_trans_block(self.num_filter*16, self.num_filter*8, act_fn)\n",
        "                self.up_1 = conv_block_2(self.num_filter*16, self.num_filter*8, act_fn)\n",
        "                self.trans_2 = conv_trans_block(self.num_filter*8, self.num_filter*4, act_fn)\n",
        "                self.up_2 = conv_block_2(self.num_filter*8, self.num_filter*4, act_fn)\n",
        "                self.trans_3 = conv_trans_block(self.num_filter*4, self.num_filter*2, act_fn)\n",
        "                self.up_3 = conv_block_2(self.num_filter*4, self.num_filter*2, act_fn)\n",
        "                self.trans_4 = conv_trans_block(self.num_filter*2, self.num_filter*1, act_fn)\n",
        "                self.up_4 = conv_block_2(self.num_filter*2, self.num_filter*1, act_fn)\n",
        "\n",
        "        def forward(self, input):\n",
        "\n",
        "                down_1 = self.down_1(input)\n",
        "                pool_1 = self.pool_1(down_1)\n",
        "                down_2 = self.down_2(pool_1)\n",
        "                pool_2 = self.pool_2(down_2)\n",
        "                down_3 = self.down_3(pool_2)\n",
        "                pool_3 = self.pool_3(down_3)\n",
        "                down_4 = self.down_4(pool_3)\n",
        "                pool_4 = self.pool_4(down_4)\n",
        "\n",
        "                bridge = self.bridge(pool_4)\n",
        "\n",
        "                trans_1 = self.trans_1(bridge)\n",
        "                concat_1 = torch.cat([trans_1, down_4], dim=1)\n",
        "                up_1 = self.up_1(concat_1)\n",
        "                trans_2 = self.trans_2(up_1)\n",
        "                concat_2 = torch.cat([trans_2, down_3], dim=1)\n",
        "                up_2 = self.up_2(concat_2)\n",
        "                trans_3 = self.trans_3(up_2)\n",
        "                concat_3 = torch.cat([trans_3, down_2], dim=1)\n",
        "                up_3 = self.up_3(concat_3)\n",
        "                trans_4 = self.trans_4(up_3)\n",
        "                concat_4 = torch.cat([trans_4, down_1], dim=1)\n",
        "                up_4 = self.up_4(concat_4)\n",
        "\n",
        "                return up_4\n",
        "\n",
        "#################### MODEL #######################################\n",
        "\n",
        "class UnetGenerator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, num_filter):\n",
        "        super(UnetGenerator, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.num_filter = num_filter\n",
        "        self.unet_first_part_1 = unet_without_last_layers_1(self.in_dim, self.out_dim, self.num_filter)\n",
        "        self.unet_first_part_2 = unet_without_last_layers_2(self.in_dim, self.out_dim, self.num_filter)\n",
        "        self.layer_out = nn.Sequential(\n",
        "            nn.Conv2d(self.num_filter, self.out_dim, 3, 1, 1),\n",
        "            nn.Tanh(),)\n",
        "\n",
        "    def forward(self, input_image, input_depth):\n",
        "\n",
        "        up_4_image = self.unet_first_part_1(input_image)\n",
        "        up_4_depth = self.unet_first_part_2(input_depth)\n",
        "        up_4_merged = torch.cat((up_4_image, up_4_depth), 1)\n",
        "        out = self.layer_out(up_4_image)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "AxTYMGYqGSlf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "generator = UnetGenerator(3, 4, 64).to(device)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--num_gpu\", type=int, default=0, help=\"number of gpus\")\n",
        "parser.add_argument(\"-f\", required=False)\n",
        "args = parser.parse_args()\n",
        "\n",
        "# load pretrained model if it is there\n",
        "print(\"loading unet model...\")\n",
        "file_model = '/content/drive/MyDrive/VPPROJECT/50epochs_depth/unet.pkl'\n",
        "if os.path.isfile(file_model):\n",
        "    generator = torch.load(file_model, map_location=torch.device('cpu'))\n",
        "    print(\"    - model restored from file....\")\n",
        "    print(\"    - filename = %s\" % file_model)\n",
        "else:\n",
        "    print(\"unable to load unet.pkl model file\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "hjAkPt4wXam5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Depth map computation with MiDas"
      ],
      "metadata": {
        "id": "05ZgMcdzTgk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration of MiDas for the depth map computation\n",
        "\n",
        "model_type = \"DPT_Hybrid\"\n",
        "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
        "midas.to(device)\n",
        "midas.eval()\n",
        "\n",
        "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "transform = midas_transforms.dpt_transform   # version of MiDas: \"DPT_Hybrid\""
      ],
      "metadata": {
        "id": "4_SqYo3tQl25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_depth_map_midas(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = transform(img).to(device)\n",
        "    depth_map = midas(img)\n",
        "    depth_map = torch.nn.functional.interpolate(\n",
        "             depth_map.unsqueeze(1),\n",
        "             size=[384, 768],\n",
        "             mode=\"bicubic\",\n",
        "             align_corners=False,\n",
        "             ).squeeze()\n",
        "    depth = depth_map.cpu().detach().numpy()\n",
        "    normalized_depth = (depth - np.min(depth)) / (np.max(depth) - np.min(depth))\n",
        "    depth_rgb = cv2.applyColorMap((normalized_depth * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
        "    return depth_rgb, torch.from_numpy(depth_rgb)\n",
        "\n",
        "# test_model:\n",
        "# depth_img = compute_depth_map_midas(\"/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/stuttgart_02_short/0010.png\")\n",
        "# cv2_imshow(depth_img)"
      ],
      "metadata": {
        "id": "hd7t1gBCHkTK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate segmented frames with Unet"
      ],
      "metadata": {
        "id": "dWIyged-TmvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n(predicting all the segmented frames will take a while)...\")\n",
        "\n",
        "# load images from folder\n",
        "folder_path = \"/content/drive/MyDrive/VPPROJECT/EUREKA/video_making/demoVideo/stuttgart_02\"\n",
        "segmented_images = []\n",
        "i = 0\n",
        "for file in os.listdir(folder_path):\n",
        "      i+=1\n",
        "      img_path = os.path.join(folder_path, file)\n",
        "      depth_map = compute_depth_map_midas(img_path)[1]\n",
        "      depth_map = depth_map.permute(2,0,1)\n",
        "      image = Image.open(img_path).convert('RGB')\n",
        "      transform_image = torchvision.transforms.Compose([\n",
        "                                    torchvision.transforms.Resize(size=(128, 256), interpolation=Image.BILINEAR),\n",
        "                                    torchvision.transforms.ToTensor()])\n",
        "      transform_depth = torchvision.transforms.Compose([\n",
        "                                    torchvision.transforms.Resize(size=(128, 256), interpolation=Image.BILINEAR)])\n",
        "      image = transform_image(image).unsqueeze(dim=0)\n",
        "      depth_map = transform_depth(depth_map).unsqueeze(dim=0)\n",
        "      image = Variable(image).float().to(device)\n",
        "      depth_map = Variable(depth_map).float().to(device)\n",
        "      y = generator.forward(image, depth_map)\n",
        "      pred_class = torch.zeros((y.size()[0], y.size()[2], y.size()[3]))\n",
        "      for idx in range(0, y.size()[0]):\n",
        "          pred_class[idx] = torch.argmax(y[idx], dim=0).to(device).int()\n",
        "      pred_class = pred_class.unsqueeze(1).float()\n",
        "      v_utils.save_image(pred_class.float()/y.size()[1], '/content/drive/MyDrive/VPPROJECT/50epochs_depth/segmented_frames/gen_image_{}.png'.format(i))"
      ],
      "metadata": {
        "id": "aMaVqCVTa6a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build video"
      ],
      "metadata": {
        "id": "m_xhDGrDTr6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build video from folder segmented_video\n",
        "segmented_video_unet_path = \"/content/drive/MyDrive/VPPROJECT/50epochs_depth/segmented_frames/\"\n",
        "built_video_unet_path = \"/content/drive/MyDrive/VPPROJECT/50epochs_depth/video_built/\"\n",
        "generate_video(segmented_video_unet_path, \"segmented_video_unet_nodepth.avi\", built_video_unet_path)\n",
        "print(\"The segmented video has been generated now!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRePBYVB1SVO",
        "outputId": "5e400a7b-cdc7-4aee-d51c-a73df8fd8256"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The segmented video has been generated now!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "- Mask-RCNN:\n",
        "https://github.com/facebookresearch/detectron2.git@5aeb252b194b93dc2879b4ac34bc51a31b5aee13'\n",
        "- Video taken from: https://www.cityscapes-dataset.com/"
      ],
      "metadata": {
        "id": "gv5vNfykb-BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@misc{wu2019detectron2,\n",
        "author = {Yuxin Wu and Alexander Kirillov and Francisco Massa and\n",
        "              Wan-Yen Lo and Ross Girshick},\n",
        "title = {Detectron2},\n",
        "howpublished = {\\url{https://github.com/facebookresearch/detectron2}},\n",
        "year = {2019}\n",
        "}"
      ],
      "metadata": {
        "id": "nE4GeQPHajSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@article{Ranftl2021,\n",
        "\tauthor    = {Ren\\'{e} Ranftl and Alexey Bochkovskiy and Vladlen Koltun},\n",
        "\ttitle     = {Vision Transformers for Dense Prediction},\n",
        "\tjournal   = {ArXiv preprint},\n",
        "\tyear      = {2021}"
      ],
      "metadata": {
        "id": "J8freC7mrd6A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}